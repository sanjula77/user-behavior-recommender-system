"""
LLM Summary API endpoints
"""

from fastapi import APIRouter, HTTPException
from src.api.models.schemas import LLMSummaryResponse
from src.api.utils.data_loader import data_loader

router = APIRouter(prefix="/summary", tags=["Summary"])


@router.get("/llm", response_model=LLMSummaryResponse)
async def get_llm_summary():
    """
    Get LLM-generated summary of insights
    
    Returns human-readable summary generated by Gemini AI including:
    - Key trends
    - Anomalies detected
    - Segment insights
    - Recommendations
    """
    try:
        data = data_loader.get_llm_summary()
        
        if not data:
            raise HTTPException(
                status_code=404,
                detail="No LLM summary found. Please run llm_summaries.py first."
            )
        
        return LLMSummaryResponse(
            summary=data.get("summary", ""),
            metadata=data.get("metadata", {}),
            timestamp=data.get("metadata", {}).get("timestamp")
        )
    
    except FileNotFoundError as e:
        raise HTTPException(
            status_code=404,
            detail=f"LLM summary not found: {str(e)}"
        )
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Error loading LLM summary: {str(e)}"
        )

